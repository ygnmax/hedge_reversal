{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565a9864-63d2-4a79-a24d-c613cadcad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "from pandas.tseries.offsets import BDay\n",
    "from scipy.optimize import minimize\n",
    "import functools as ft\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76d4ecf-ff8f-4039-a68f-bbd4925cb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if getpass.getuser() in ['ygnmax']:\n",
    "    if sys.platform == 'linux':\n",
    "        workdir = '/home/ygnmax/Dropbox/research_nyu/hedge_vol/'\n",
    "    if sys.platform == 'win32':\n",
    "        workdir = 'C:/Users/ygnmax/Dropbox (Personal)/research_nyu/hedge_vol/'\n",
    "\n",
    "os.chdir(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1acb9ef-b36c-4e12-8f3d-4e98f712f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# 1. Get stock list / dictionary\n",
    "#################################\n",
    "# read dot-com bubble companies\n",
    "df_dotcom = pd.read_excel(workdir + \"data/dot_com_firms.xlsx\", engine = 'openpyxl').dropna()\n",
    "df_dotcom[\"SecurityID\"] = df_dotcom[\"SecurityID\"].astype(int)\n",
    "\n",
    "# read big companies\n",
    "df_big = pd.read_excel(workdir + \"data/big_firms.xlsx\", engine = 'openpyxl').dropna()\n",
    "df_big[\"SecurityID\"] = df_big[\"SecurityID\"].astype(int)\n",
    "\n",
    "# read other companies\n",
    "df_other = pd.read_excel(workdir + \"data/other_firms.xlsx\").dropna()\n",
    "df_other[\"SecurityID\"] = df_other[\"SecurityID\"].astype(int)\n",
    "\n",
    "# append them together\n",
    "df_stock_list = pd.concat([df_dotcom[['Name','Ticker','SecurityID', 'Internet']], \n",
    "                           df_big[['Name','Ticker','SecurityID', 'Internet']], \n",
    "                          df_other[['Name','Ticker','SecurityID', 'Internet']]])\n",
    "# df_stock_list = pd.concat([df_stock_list, df_other[['Name','Ticker','SecurityID', 'Internet']]])\n",
    "df_stock_list[\"Internet\"] = df_stock_list[\"Internet\"].astype(int)\n",
    "df_stock_list = df_stock_list.dropna()\n",
    "df_stock_list = df_stock_list.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d93df0-f2dd-43fa-bb30-7a938a474b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_iv_core(idx, group):\n",
    "    dict_IV_rows = {} \n",
    "    for j, row in group.iterrows():\n",
    "        if row.impl_cdiv_median == 0:\n",
    "            # print(j, ' dividend rate is 0')\n",
    "            dict_IV_rows[j] = row.IV0\n",
    "        else:\n",
    "            date = row.Date\n",
    "            S = row.S0\n",
    "            X = row.K\n",
    "            T = row.Maturity/360.0\n",
    "            r_0 = row.r\n",
    "            q_0 = row.impl_cdiv_median\n",
    "            IV0 = row.IV0            \n",
    "            if np.isnan(IV0.values):\n",
    "                IV0 = 1.5\n",
    "            if row.CallPut == 'C':    \n",
    "                def f(x):\n",
    "                    return (ABM(ft.partial(VP, K=X, CallPut='C'), S, T, r_0, x, 1000, q_0)-row.V0)**2\n",
    "            else:\n",
    "                def f(x):\n",
    "                    return (ABM(ft.partial(VP,K=X, CallPut='P'), S, T, r_0, x, 1000, q_0)-row.V0)**2\n",
    "            # Optimizing\n",
    "            cons = ({'type': 'ineq', 'fun' : lambda x: np.array(x), 'jac': lambda x: np.array([1.0])})\n",
    "            res = minimize(f, np.array([IV0]), constraints=cons, tol = 0.0001)\n",
    "            dict_IV_rows[j] = res.x[0]\n",
    "        \n",
    "    df_IV_rows = pd.DataFrame(data = dict_IV_rows.items(), columns = ['index', 'IV0_c'])\n",
    "    df_IV_rows = df_IV_rows.set_index(['index'])\n",
    "    df_IV_rows_out = group.merge(df_IV_rows, left_index = True, right_index = True, how = 'left')\n",
    "    dict_IV[idx] = df_IV_rows_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f9252e-2d01-4553-b60b-f7bc8cd627ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%run \"src/meme_stocks/functions_greeks.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97df5611-45a0-4a5d-ae7b-ac8abb9d73d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "processing 113993 with 210872 rows\n"
     ]
    }
   ],
   "source": [
    "input_path = workdir + 'data/processed/intermediate/WRDS_2021/'\n",
    "\n",
    "i = 100\n",
    "stkid = df_stock_list.loc[i, 'SecurityID']\n",
    "ticker = df_stock_list.loc[i, 'Ticker']\n",
    "    \n",
    "df = pd.read_csv(input_path + \"df_\" + str(stkid) + \".csv\", parse_dates = ['Date']) \n",
    "df['impl_cdiv_median']=df.loc[:,['impl_cdiv30', 'impl_cdiv60', 'impl_cdiv90']].median(axis=1)\n",
    "\n",
    "print('***************************************')\n",
    "print('processing '+ str(stkid) + ' with ' + str(len(df)) + ' rows')\n",
    "\n",
    "row = df.loc[[52718], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70b5ec-92fb-43f2-9253-8a95f3fbb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = row.Date.values\n",
    "# S = row.S0.values\n",
    "# X = row.K.values\n",
    "# T = row.Maturity.values/360.0\n",
    "# r_0 = row.r.values\n",
    "# q_0 = row.impl_cdiv_median.values\n",
    "# IV0 = row.IV0   \n",
    "# if np.isnan(IV0.values):\n",
    "#     IV0 = 1.5\n",
    "    \n",
    "# if row.CallPut.values == 'C':    \n",
    "#     def f(x):\n",
    "#         return (ABM(ft.partial(VP, K=X, CallPut='C'), S, T, r_0, x, 1000, q_0)-row.V0)**2\n",
    "# else:\n",
    "#     def f(x):\n",
    "#         return (ABM(ft.partial(VP,K=X, CallPut='P'), S, T, r_0, x, 1000, q_0)-row.V0)**2\n",
    "\n",
    "# cons = ({'type': 'ineq', 'fun' : lambda x: np.array(x), 'jac': lambda x: np.array([1.0])})\n",
    "# res = minimize(f, np.array([7.5]), constraints=cons, tol = 0.0001)\n",
    "# res.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2975b6ff-6042-4df7-9ff1-50808d887c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "processing 113993 with 210872 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   38.9s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed: 24.1min\n",
      "[Parallel(n_jobs=8)]: Done 2434 tasks      | elapsed: 31.3min\n",
      "[Parallel(n_jobs=8)]: Done 3184 tasks      | elapsed: 41.1min\n",
      "[Parallel(n_jobs=8)]: Done 4034 tasks      | elapsed: 64.1min\n",
      "[Parallel(n_jobs=8)]: Done 4984 tasks      | elapsed: 87.6min\n",
      "[Parallel(n_jobs=8)]: Done 6034 tasks      | elapsed: 324.6min\n",
      "[Parallel(n_jobs=8)]: Done 7184 tasks      | elapsed: 679.3min\n",
      "[Parallel(n_jobs=8)]: Done 8434 tasks      | elapsed: 902.3min\n",
      "[Parallel(n_jobs=8)]: Done 8657 out of 8657 | elapsed: 936.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113993 implied volatility within 56189.059601306915 seconds\n"
     ]
    }
   ],
   "source": [
    "input_path = workdir + 'data/processed/intermediate/WRDS_2021/'\n",
    "output_path = input_path\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "problem_stk_list = []\n",
    "for i in [100]: # list(df_stock_list.index):\n",
    "    stkid = df_stock_list.loc[i, 'SecurityID']\n",
    "    ticker = df_stock_list.loc[i, 'Ticker']\n",
    "    \n",
    "    if os.path.exists(output_path + '/iv_' + str(stkid) + '.csv') == True:\n",
    "        print('***************************************')\n",
    "        print('Stock '+ str(stkid) + ' IV has existed already')        \n",
    "        continue\n",
    "    if os.path.exists(input_path + \"df_\" + str(stkid) + \".csv\") == False:\n",
    "        print('*****************************')\n",
    "        print('Stock '+ str(stkid) + ' is not available') \n",
    "        problem_stk_list.append(stkid)\n",
    "        continue\n",
    "    else:     \n",
    "        df = pd.read_csv(input_path + \"df_\" + str(stkid) + \".csv\", parse_dates = ['Date']) \n",
    "        try:\n",
    "            df['impl_cdiv_median']=df.loc[:,['impl_cdiv30', 'impl_cdiv60', 'impl_cdiv90']].median(axis=1)\n",
    "\n",
    "            print('***************************************')\n",
    "            print('processing '+ str(stkid) + ' with ' + str(len(df)) + ' rows')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(df) < 2:\n",
    "        problem_stk_list.append(stkid)\n",
    "        print('Stock '+ str(stkid) + ticker + ' has insufficient observations')\n",
    "        continue  \n",
    "#     elif len(df) > 20000:\n",
    "#         print('Stock '+ str(stkid) + ticker + ' has too many observations, calculate them later')\n",
    "#         continue\n",
    "    else:     \n",
    "        ###############################\n",
    "        # Calculate Implied Volatility\n",
    "        ###############################\n",
    "        start_time = time.time()\n",
    "        dict_IV = {}\n",
    "        Parallel(n_jobs=8, require='sharedmem', verbose=1)(delayed(cal_iv_core)(idx, group) for idx, group in df.groupby(['Date', 'Expiration']))  \n",
    "        print(str(stkid) + ' implied volatility within %s seconds' % (time.time()-start_time))\n",
    "        df_IV_out = pd.DataFrame()\n",
    "        for key, val in dict_IV.items():\n",
    "            df_IV_out = pd.concat([df_IV_out, dict_IV[key]])\n",
    "            \n",
    "        df_IV_out.to_csv(output_path + '/iv_' + str(stkid) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5514ad9-1577-488b-b8dd-6de0a7c092ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_delta_core(idx, group):\n",
    "    dict_V_self = {} \n",
    "    dict_V_self_up = {} \n",
    "    dict_V_self_down = {} \n",
    "    for j, row in group.iterrows():\n",
    "        date = row.Date\n",
    "        S = row.S0\n",
    "        X = row.K\n",
    "        T = row.Maturity/360.0\n",
    "        IV0 = row.IV0_c\n",
    "        r_0 = row.r\n",
    "        q_0 = row.impl_cdiv_median\n",
    "\n",
    "        epsilon = 0.01\n",
    "        S_epsilon_up = S * (1+epsilon)\n",
    "        S_epsilon_down = S * (1-epsilon)\n",
    "\n",
    "        if row.CallPut == 'C':\n",
    "            V_self = ABM(ft.partial(VP,K=X,CallPut='C'), S, T, r_0, IV0, 1000, q_0)                 \n",
    "            V_self_epsilon_up = ABM(ft.partial(VP,K=X,CallPut='C'), S_epsilon_up, T, r_0, IV0, 1000, q_0) \n",
    "            V_self_epsilon_download = ABM(ft.partial(VP,K=X,CallPut='C'), S_epsilon_down, T, r_0, IV0, 1000, q_0) \n",
    "            dict_V_self[j] = V_self\n",
    "            dict_V_self_up[j] = V_self_epsilon_up\n",
    "            dict_V_self_down[j] = V_self_epsilon_download\n",
    "        else:\n",
    "            V_self = ABM(ft.partial(VP,K=X,CallPut='P'), S, T, r_0, IV0, 1000, q_0)                 \n",
    "            V_self_epsilon_up = ABM(ft.partial(VP,K=X,CallPut='P'), S_epsilon_up, T, r_0, IV0, 1000, q_0) \n",
    "            V_self_epsilon_download = ABM(ft.partial(VP,K=X,CallPut='P'), S_epsilon_down, T, r_0, IV0, 1000, q_0) \n",
    "            dict_V_self[j] = V_self\n",
    "            dict_V_self_up[j] = V_self_epsilon_up\n",
    "            dict_V_self_down[j] = V_self_epsilon_download\n",
    "    \n",
    "    df_V_self = pd.DataFrame(data = dict_V_self.items(), columns = ['index', 'V_self'])\n",
    "    df_V_self_epsilon_up = pd.DataFrame(data = dict_V_self_up.items(), columns = ['index', 'V_up'])\n",
    "    df_V_self_epsilon_down = pd.DataFrame(data = dict_V_self_down.items(), columns = ['index', 'V_down'])\n",
    "    \n",
    "    df_V_self = df_V_self.set_index(['index'])\n",
    "    df_V_self_epsilon_up = df_V_self_epsilon_up.set_index(['index'])\n",
    "    df_V_self_epsilon_down = df_V_self_epsilon_down.set_index(['index'])\n",
    "    \n",
    "    df_V_out = group.merge(df_V_self, left_index = True, right_index = True, how = 'left')\n",
    "    df_V_out = df_V_out.merge(df_V_self_epsilon_up, left_index = True, right_index = True, how = 'left')\n",
    "    df_V_out = df_V_out.merge(df_V_self_epsilon_down, left_index = True, right_index = True, how = 'left')\n",
    "    dict_delta[idx] = df_V_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "959a4387-ea3e-4273-befc-8cd53e661ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "processing 113993 with 210872 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=8)]: Done 2434 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=8)]: Done 3184 tasks      | elapsed: 23.4min\n",
      "[Parallel(n_jobs=8)]: Done 4034 tasks      | elapsed: 35.0min\n",
      "[Parallel(n_jobs=8)]: Done 4984 tasks      | elapsed: 50.2min\n",
      "[Parallel(n_jobs=8)]: Done 6034 tasks      | elapsed: 142.1min\n",
      "[Parallel(n_jobs=8)]: Done 7184 tasks      | elapsed: 281.2min\n",
      "[Parallel(n_jobs=8)]: Done 8434 tasks      | elapsed: 375.6min\n",
      "[Parallel(n_jobs=8)]: Done 8657 out of 8657 | elapsed: 391.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113993 delta within 23479.45998096466 seconds\n"
     ]
    }
   ],
   "source": [
    "input_path = workdir + 'data/processed/intermediate/WRDS_2021/'\n",
    "output_path = input_path\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "problem_stk_list = []\n",
    "for i in [100]: # list(df_stock_list.index):\n",
    "    stkid = df_stock_list.loc[i, 'SecurityID']\n",
    "    ticker = df_stock_list.loc[i, 'Ticker']\n",
    "    \n",
    "    # if os.path.exists(output_path + '/delta_' + str(stkid) + '.csv') == True:\n",
    "    #     print('***************************************')\n",
    "    #     print('Stock '+ str(stkid) + ' delta has existed already')        \n",
    "    #     continue\n",
    "    if os.path.exists(input_path + \"iv_\" + str(stkid) + \".csv\") == False:\n",
    "        print('*****************************')\n",
    "        print('Stock '+ str(stkid) + ' is not available') \n",
    "        problem_stk_list.append(stkid)\n",
    "        continue\n",
    "    else:     \n",
    "        df = pd.read_csv(input_path + \"iv_\" + str(stkid) + \".csv\", parse_dates = ['Date']) \n",
    "        print('***************************************')\n",
    "        print('processing '+ str(stkid) + ' with ' + str(len(df)) + ' rows')\n",
    "            \n",
    "    if len(df) < 2:\n",
    "        problem_stk_list.append(stkid)\n",
    "        print('Stock '+ str(stkid) + ticker + ' has insufficient observations')\n",
    "        continue     \n",
    "#     elif len(df) > 20000:\n",
    "#         print('Stock '+ str(stkid) + ticker + ' has too many observations, calculate them later')\n",
    "#         continue        \n",
    "    else:     \n",
    "        ##################\n",
    "        # Calculate Delta\n",
    "        ################## \n",
    "        start_time = time.time()\n",
    "        dict_delta = {}\n",
    "        Parallel(n_jobs=8, require='sharedmem', verbose=1)(delayed(cal_delta_core)(idx, group) for idx, group in df.groupby(['Date', 'Expiration']))  \n",
    "        print(str(stkid) + ' delta within %s seconds' % (time.time()-start_time))\n",
    "\n",
    "        df_delta_out = pd.DataFrame()\n",
    "        for key, val in dict_delta.items():\n",
    "            df_delta_out = pd.concat([df_delta_out, dict_delta[key]])\n",
    "\n",
    "        df_delta_out['Delta_c'] = (df_delta_out['V_up'] - df_delta_out['V_down']) / (df_delta_out['S0'] * 0.02)\n",
    "        df_delta_out.to_csv(output_path + '/delta_'+ str(stkid) + '.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae6f55a9-dfcc-4221-9958-414a525a607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_delta_out.loc[5095, :]\n",
    "date = row.Date\n",
    "S = row.S0\n",
    "X = row.K\n",
    "T = row.Maturity/360.0\n",
    "IV0 = row.IV0_c\n",
    "r_0 = row.r\n",
    "q_0 = row.impl_cdiv_median\n",
    "\n",
    "epsilon = 0.01\n",
    "S_epsilon_up = S * (1+epsilon)\n",
    "S_epsilon_down = S * (1-epsilon)\n",
    "\n",
    "V_self = ABM(ft.partial(VP,K=X,CallPut='P'), S, T, r_0, IV0, 1000, q_0)                 \n",
    "V_self_epsilon_up = ABM(ft.partial(VP,K=X,CallPut='P'), S_epsilon_up, T, r_0, IV0, 1000, q_0) \n",
    "V_self_epsilon_download = ABM(ft.partial(VP,K=X,CallPut='P'), S_epsilon_down, T, r_0, IV0, 1000, q_0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e6510-d7ba-406d-9581-67ad0f7e8797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
