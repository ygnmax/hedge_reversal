{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pandas.tseries.offsets import BDay\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if getpass.getuser() in ['ygnmax']:\n",
    "    if sys.platform == 'linux':\n",
    "        workdir = '/home/ygnmax/Dropbox/research_nyu/hedge_vol/'\n",
    "    if sys.platform == 'win32':\n",
    "        workdir = 'C:/Users/ygnmax/Dropbox/research_nyu/hedge_vol/'\n",
    "os.chdir(workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_hedge(df_input):\n",
    "    df = df_input.copy()\n",
    "    #----------------------------\n",
    "    # Calculate Hedge PNL/Error\n",
    "    #----------------------------\n",
    "    df['Zero_Hedge_PL'] = df['V0_n']*df['on_ret'] - df['V1_n']\n",
    "    df_C = df[(df['CallPut'] == 'C')]\n",
    "    df_P = df[(df['CallPut'] == 'P')] \n",
    "    NoHedge_C = np.mean(df_C['Zero_Hedge_PL'] ** 2)\n",
    "    NoHedge_P = np.mean(df_P['Zero_Hedge_PL'] ** 2)\n",
    "    return NoHedge_C, NoHedge_P\n",
    "\n",
    "def BS_hedge(df_input, delta_var):\n",
    "    df = df_input.copy()\n",
    "    df['BS_PL'] = df[delta_var] * df['S1_n'] + df['on_ret'] * (df['V0_n'] - df[delta_var] * df['S0_n']) - df['V1_n']\n",
    "    df_C = df[df['CallPut'] == 'C']\n",
    "    df_P = df[df['CallPut'] == 'P']      \n",
    "    BSHedge_C = np.mean(df_C.loc[:,'BS_PL'] ** 2)\n",
    "    BSHedge_P = np.mean(df_P.loc[:,'BS_PL'] ** 2)\n",
    "    return BSHedge_C, BSHedge_P\n",
    "\n",
    "def Fixed_BS_Hedge(df_input, delta_var, call_coef = 0.9, put_coef = 1.1, whole_period = False):\n",
    "    df = df_input.copy()    \n",
    "    df.loc[df['CallPut'] == 'C', 'DeltaFixed'] = call_coef * df[delta_var]\n",
    "    df.loc[df['CallPut'] == 'P', 'DeltaFixed'] = put_coef * df[delta_var]\n",
    "    df['Fixed_PL'] = df['DeltaFixed'] * df['S1_n'] + df['on_ret'] * (df['V0_n'] - df['DeltaFixed'] * df['S0_n']) - df['V1_n']\n",
    "    df_C = df[(df['CallPut'] == 'C')]\n",
    "    df_P = df[(df['CallPut'] == 'P')]\n",
    "    \n",
    "    FixedHedge_C = np.mean(df_C['Fixed_PL']** 2)\n",
    "    FixedHedge_P = np.mean(df_P['Fixed_PL'] ** 2)\n",
    "\n",
    "    return FixedHedge_C, FixedHedge_P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pnl(train, test, weight, delta_var = 'Delta'): \n",
    "    df_train = train.copy()\n",
    "    df_test = test.copy()\n",
    "    df_weight = weight.copy()\n",
    "    \n",
    "    delta_hat = pd.Series(index=df_test.index, dtype = 'float64')\n",
    "    dict_output = {}\n",
    "    dict_plot = {}\n",
    "    \n",
    "    if len(np.unique(df_train['CallPut'])) == 1:\n",
    "        if np.unique(df_train['CallPut'])[0] == 'C':\n",
    "            df_train.loc[-1, 'CallPut'] = 'P'\n",
    "        else:\n",
    "            df_train.loc[-1, 'CallPut'] = 'C'\n",
    "    \n",
    "    for optype, group in df_train.groupby(by=['CallPut']):\n",
    "        test_index = df_test.loc[df_test['CallPut'] == optype].index\n",
    "        dict_coef = {}        \n",
    "        if len(group) < 2:\n",
    "            print('training set of ' + optype + ' has insufficient observations')\n",
    "            coef = [np.nan]\n",
    "            std = np.nan\n",
    "            pass\n",
    "        else:\n",
    "            ## add weights\n",
    "            group = pd.merge(group, df_weight, how = 'left', on = 'Date')\n",
    "            earliest = group.loc[group['Date'] == np.min(group['Date'].values), :].copy()\n",
    "             \n",
    "            ## y_train is the change of option price in the training set\n",
    "            y_train = group['V1_n'] - group['V0_n'] * group['on_ret']\n",
    "            ## x_train is Delta times the change of stock price in the training set\n",
    "            w_train = group.loc[:, 'weight'].copy()\n",
    "            x_train = group.loc[:, delta_var].copy()\n",
    "            x_train = x_train.multiply(group['S1_n'] - group['S0_n'] * group['on_ret'], axis=0).values.reshape(-1,1)\n",
    "            lin = LinearRegression(fit_intercept=False).fit(x_train, y_train, sample_weight = group['weight'])\n",
    "            coef = lin.coef_\n",
    "            \n",
    "            y_earliest = earliest['V1_n'] - earliest['V0_n'] * earliest['on_ret']\n",
    "            x_earliest = earliest.loc[:, delta_var].copy()\n",
    "            x_earliest = x_earliest.multiply(earliest['S1_n'] - earliest['S0_n'] * earliest['on_ret'], axis=0).values.reshape(-1,1)\n",
    "            w_earliest = earliest.loc[:, 'weight'].copy()\n",
    "            \n",
    "            dict_plot[optype + '_x_train'] = x_train\n",
    "            dict_plot[optype + '_y_train'] = y_train\n",
    "            dict_plot[optype + '_w_train'] = w_train\n",
    "            dict_plot[optype + '_coef'] = coef\n",
    "            dict_plot[optype + '_x_earliest'] = x_earliest\n",
    "            dict_plot[optype + '_y_earliest'] = y_earliest\n",
    "            dict_plot[optype + '_w_earliest'] = w_earliest\n",
    "            \n",
    "            ## calculate the standard error of coefficient\n",
    "            y_hat_train = lin.predict(x_train)\n",
    "            residual_sum_of_square = ((y_train - y_hat_train) ** 2).sum()\n",
    "            sigma_square_hat = residual_sum_of_square / (x_train.shape[0] - x_train.shape[1])\n",
    "            var_beta = (np.linalg.inv(x_train.T @ x_train) * sigma_square_hat)\n",
    "            std = [np.sqrt(var_beta[i, i]) for i in range(len(var_beta))]\n",
    "            \n",
    "            \n",
    "            ## y_hat_test is predicted delta in the test set \n",
    "            if len(df_test[(df_test['CallPut'] == optype)]) < 1:\n",
    "                print('test set of ' + optype + ' has insufficient observations')\n",
    "                pass\n",
    "            else:\n",
    "                delta_hat.loc[test_index] = lin.predict(df_test.loc[test_index, delta_var].values.reshape(-1,1))\n",
    "                \n",
    "                y_test = df_test.loc[test_index, 'V1_n'] - df_test.loc[test_index, 'V0_n'] * df_test.loc[test_index, 'on_ret']\n",
    "                x_test = df_test.loc[test_index, delta_var].copy()\n",
    "                x_test = x_test.multiply(df_test.loc[test_index, 'S1_n'] - df_test.loc[test_index, 'S0_n'] * df_test.loc[test_index, 'on_ret'], axis=0).values.reshape(-1,1)  \n",
    "\n",
    "                dict_plot[optype + '_x_test'] = x_test\n",
    "                dict_plot[optype + '_y_test'] = y_test\n",
    "                dict_plot[optype + '_predict'] = lin.predict(x_test)   \n",
    "            \n",
    "        dict_coef['type'] = optype\n",
    "        dict_coef['coef'] = coef\n",
    "        dict_coef['std'] = std\n",
    "        dict_coef['N_train'] = len(group)\n",
    "        dict_coef['days_train'] = len(np.unique(group.Date))\n",
    "        dict_coef['N_test'] = len(df_test.loc[test_index])\n",
    "        dict_coef['days_test'] = len(np.unique(df_test.loc[test_index].Date))\n",
    "                \n",
    "        df_test_atm = df_test.loc[(df_test['CallPut'] == optype), :]\n",
    "        if len(df_test_atm) == 0:\n",
    "            atm30_vol = np.NaN\n",
    "            atm30_v = np.NaN\n",
    "            atm60_vol = np.NaN\n",
    "            atm60_v = np.NaN\n",
    "            atm90_vol = np.NaN\n",
    "            atm90_v = np.NaN\n",
    "            pass\n",
    "        else:\n",
    "            atm30_vol = np.unique(df_test_atm.loc[:, 'IV_ATM30'])[0]\n",
    "            atm30_v = np.unique(df_test_atm.loc[:, 'V_ATM30'])[0]\n",
    "            if (atm30_vol < 0.01) | (atm30_vol > 5):\n",
    "                atm30_vol = np.NaN\n",
    "            atm60_vol = np.unique(df_test_atm.loc[:, 'IV_ATM60'])[0]\n",
    "            atm60_v = np.unique(df_test_atm.loc[:, 'V_ATM60'])[0]\n",
    "            if (atm60_vol < 0.01) | (atm60_vol > 5):\n",
    "                atm60_vol = np.NaN\n",
    "            atm90_vol = np.unique(df_test_atm.loc[:, 'IV_ATM90'])[0]\n",
    "            atm90_v = np.unique(df_test_atm.loc[:, 'V_ATM90'])[0]\n",
    "            if (atm60_vol < 0.01) | (atm60_vol > 5):\n",
    "                atm90_vol = np.NaN\n",
    "       \n",
    "        dict_coef['atm30_vol_test'] = atm30_vol\n",
    "        dict_coef['atm30_oprice_test'] = atm30_v\n",
    "        dict_coef['atm60_vol_test'] = atm60_vol\n",
    "        dict_coef['atm60_oprice_test'] = atm60_v        \n",
    "        dict_coef['atm90_vol_test'] = atm90_vol\n",
    "        dict_coef['atm90_oprice_test'] = atm90_v \n",
    "               \n",
    "        dict_coef['list_maturity_train'] = np.unique(group.Maturity)\n",
    "        dict_output[optype] = dict_coef\n",
    "\n",
    "    #------------------------------------------\n",
    "    ## calculate PNL using the estimated delta\n",
    "    #------------------------------------------\n",
    "    pnl = delta_hat * df_test['S1_n'] + (df_test['V0_n'] - delta_hat * df_test['S0_n']) * df_test['on_ret']  - df_test['V1_n']  \n",
    "\n",
    "    df_PNL = df_test[['Date', 'CallPut']].copy()\n",
    "    df_PNL['delta'] = delta_hat\n",
    "    df_PNL['PNL'] = pnl\n",
    "    df_PNL['M0'] = df_test['M0'].copy()\n",
    "    df_PNL['tau0'] = df_test['tau0'].copy()\n",
    "    df_PNL['OptionID'] = df_test['OptionID'].copy()\n",
    "    return df_PNL, dict_output, dict_plot, group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_syn(df_input, stock_id):\n",
    "    df = df_input.copy()\n",
    "    df_syn = pd.read_csv(workdir + 'data/processed/synthetic/syn_WRDS_2021/df_' + str(stock_id) + '.csv', parse_dates = ['Date', 'Expiration'])\n",
    "    df_syn_30 = df_syn.loc[df_syn['Maturity'] == 30, ['Date', 'CallPut', 'StockPrice', 'OptionPrice', 'IV']].rename(columns = {'IV': 'IV_ATM30', 'OptionPrice': 'V_ATM30'})\n",
    "    df_syn_60 = df_syn.loc[df_syn['Maturity'] == 60, ['Date', 'CallPut', 'OptionPrice', 'IV']].rename(columns = {'IV': 'IV_ATM60', 'OptionPrice': 'V_ATM60'})\n",
    "    df_syn_90 = df_syn.loc[df_syn['Maturity'] == 90, ['Date', 'CallPut', 'OptionPrice', 'IV']].rename(columns = {'IV': 'IV_ATM90', 'OptionPrice': 'V_ATM90'})\n",
    "\n",
    "    df_syn = pd.merge(df_syn_30, df_syn_60, on = ['Date', 'CallPut'], how = 'left')\n",
    "    df_syn = df_syn.merge(df_syn_90, on = ['Date', 'CallPut'], how = 'left')\n",
    "\n",
    "    df = df.merge(df_syn, how = 'left', on = ['Date', 'CallPut'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Get stock list / dictionary\n",
    "##############################\n",
    "df_bubble = pd.read_excel(workdir + \"data/dot_com_firms.xlsx\", engine = 'openpyxl').dropna()\n",
    "df_bubble[\"SecurityID\"] = df_bubble[\"SecurityID\"].astype(int)\n",
    "df_big = pd.read_excel(workdir + \"data/big_firms.xlsx\", engine = 'openpyxl').dropna()\n",
    "df_big[\"SecurityID\"] = df_big[\"SecurityID\"].astype(int)\n",
    "# read other companies\n",
    "df_other = pd.read_excel(workdir + \"data/other_firms.xlsx\").dropna()\n",
    "df_other[\"SecurityID\"] = df_other[\"SecurityID\"].astype(int)\n",
    "\n",
    "df_stock_list = pd.concat([df_bubble[['Name','Ticker','SecurityID', 'Internet']], \n",
    "                           df_big[['Name','Ticker','SecurityID', 'Internet']], \n",
    "                          df_other[['Name','Ticker','SecurityID', 'Internet']]])\n",
    "df_stock_list[\"Internet\"] = df_stock_list[\"Internet\"].astype(int)\n",
    "df_stock_list = df_stock_list.dropna()\n",
    "df_stock_list = df_stock_list.reset_index(drop = True)\n",
    "\n",
    "ids = {}\n",
    "dict_tech_label = {}\n",
    "for i in list(df_stock_list.index):\n",
    "    ids[df_stock_list.loc[i, 'SecurityID']] = df_stock_list.loc[i, 'Name']\n",
    "    dict_tech_label[df_stock_list.loc[i, 'SecurityID']] = df_stock_list.loc[i, 'Internet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calulate the PNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "step_path = '/step_' + str(step) + 'd_'\n",
    "\n",
    "adjusted_delta = 1\n",
    "if adjusted_delta == 1:\n",
    "    adjusted_delta_path = 'adjusted_delta_'\n",
    "    input_path = workdir + 'data/processed/intermediate/WRDS_2021/' + 'delta_'\n",
    "    delta_var = 'Delta_c'\n",
    "elif adjusted_delta == 0:\n",
    "    adjusted_delta_path = 'raw_delta_'\n",
    "    input_path = workdir + 'data/processed/intermediate/WRDS_2021/' + 'df_'\n",
    "    delta_var = 'Delta'\n",
    "else:\n",
    "    print('Set the correct delta')\n",
    "\n",
    "    \n",
    "rolling_weights = True\n",
    "\n",
    "M_min = 0\n",
    "M_max = 100\n",
    "train_length = 20 # ([train_length, parameter]: [240, 0.99], [360, 0.995])\n",
    "\n",
    "if rolling_weights:\n",
    "    train_length_path = 'train_length_' + str(train_length) + 'd_wt/'\n",
    "    wt_exp = [0.99**i for i in range(train_length-1, -1, -1)]\n",
    "else:\n",
    "    train_length_path = 'train_length_' + str(train_length) + 'd/'\n",
    "    wt_exp  = [1.00**i for i in range(train_length-1, -1, -1)]\n",
    "\n",
    "output_path = workdir + 'output/regression/moneyness_' + str(M_min) + '_' + str(M_max) + step_path + adjusted_delta_path + train_length_path\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    os.makedirs(output_path + 'coef')\n",
    "    os.makedirs(output_path + 'MSHE')\n",
    "    os.makedirs(output_path + 'coefplot')\n",
    "    os.makedirs(output_path + 'PNL')\n",
    "    os.makedirs(output_path + 'PNL_plot')\n",
    "    os.makedirs(output_path + 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(dict_plot_one, securityid, date_in, output_path, version = adjusted_delta):\n",
    "    if version == 0:\n",
    "        greek_letter = 'Raw Delta'\n",
    "    else:\n",
    "        greek_letter = 'Adjusted Delta'\n",
    "\n",
    "    plt.rc('font', size=25)          # controls default text sizes\n",
    "    datestr = pd.to_datetime(date_in[0]).strftime('%Y-%m-%d')\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18,9), sharex=True, sharey=True)\n",
    "    # fig.suptitle(tickername + ' ' + str(i) + ' ' + ids[i].strip() + ' ' + datestr)\n",
    "    plt.setp(axs[:], xlabel=greek_letter + \" X Change of Stock Price\")\n",
    "    plt.setp(axs[0], ylabel=\"Change of Option Price\")\n",
    "    \n",
    "    w_C = dict_plot_one['C_w_train'] * 30\n",
    "    w_e_C = dict_plot_one['C_w_earliest'] * 30\n",
    "    axs[0].scatter(dict_plot_one['C_x_train'], dict_plot_one['C_y_train'], s = w_C)\n",
    "    axs[0].plot(dict_plot_one['C_x_train'], dict_plot_one['C_coef'] * dict_plot_one['C_x_train'], color='tab:orange')\n",
    "    axs[0].scatter(dict_plot_one['C_x_test'], dict_plot_one['C_y_test'], color='tab:red')\n",
    "    axs[0].scatter(dict_plot_one['C_x_earliest'], dict_plot_one['C_y_earliest'], s = w_e_C, color='tab:grey')\n",
    "    axs[0].annotate('Call Coefficient: ' + str(round(dict_plot_one['C_coef'][0], 4)), \n",
    "                    xy=(0, 1), xytext=(12, -12), va='top', xycoords='axes fraction', textcoords='offset points')\n",
    "    axs[0].set_title(\"Call\")\n",
    "    \n",
    "    w_P = dict_plot_one['P_w_train'] * 30\n",
    "    w_e_P = dict_plot_one['P_w_earliest'] * 30\n",
    "    axs[1].scatter(dict_plot_one['P_x_train'], dict_plot_one['P_y_train'], s = w_P)\n",
    "    axs[1].plot(dict_plot_one['P_x_train'], dict_plot_one['P_coef'] * dict_plot_one['P_x_train'], color='tab:orange')\n",
    "    axs[1].scatter(dict_plot_one['P_x_test'], dict_plot_one['P_y_test'], color='tab:red')\n",
    "    axs[1].scatter(dict_plot_one['P_x_earliest'], dict_plot_one['P_y_earliest'], s = w_e_P, color='tab:grey')\n",
    "    axs[1].annotate('Put Coefficient: ' + str(round(dict_plot_one['P_coef'][0], 4)), \n",
    "                    xy=(0, 1), xytext=(12, -12), va='top', xycoords='axes fraction', textcoords='offset points')\n",
    "    axs[1].set_title(\"Put\")   \n",
    "    fig.tight_layout()        \n",
    "\n",
    "\n",
    "    scatter_output_path = output_path + 'scatter/' + str(securityid) + '/'\n",
    "    if not os.path.exists(scatter_output_path):\n",
    "        os.makedirs(scatter_output_path)\n",
    "    plt.savefig(scatter_output_path + str(securityid) + '_' + tickername + '_' + datestr + '.jpg')\n",
    "        \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "No.1 processing 113993\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "df_MSHE_all = pd.DataFrame()\n",
    "for i in [100]: # range(100, 119, 1): #{113993: 'Game Stop', 189943: 'AMC'}:  # {107899: 'NASDAQ 100 TR', 102480: 'NASDAQ 100 INDEX', 108105: 'SPX'}: #ids: # {111020: '3Com', 108005: 'NetBank', 111860: 'Walmart'}: \n",
    "    securityid = df_stock_list.loc[i, 'SecurityID']\n",
    "    i = securityid\n",
    "    tickername = df_stock_list.loc[df_stock_list['SecurityID'] == securityid, 'Ticker'].values[0].strip()\n",
    "    j = j + 1\n",
    "    print('*********************************************')\n",
    "    print('No.' + str(j) + ' processing '+ str(i))     \n",
    "    \n",
    "    if os.path.exists(input_path + str(i) + '.csv') == False:\n",
    "        print('Stock '+ str(i) + ' is not available')\n",
    "        continue\n",
    "    else:\n",
    "        temp = pd.read_csv(input_path + str(i) + \".csv\", parse_dates = ['Date'])\n",
    "        # temp = temp.loc[temp['Date'] != pd.to_datetime(\"1998-10-12\"), :]\n",
    "        temp = temp.loc[~temp['IV0'].isna(), :]\n",
    "        #------------------\n",
    "        # Shrink moneyness\n",
    "        #------------------\n",
    "        bl = (temp['M0'] >= M_min-0.001) & (temp['M0'] <= M_max+0.001)\n",
    "        temp = temp.loc[bl] \n",
    "\n",
    "        #---------------------\n",
    "        # more cleaning\n",
    "        #---------------------\n",
    "        # continuous version rate:\n",
    "        temp['on_ret'] = np.exp(temp['short_rate'] * 1 / 253)\n",
    "        \n",
    "        bl_C = ((temp['CallPut'] == 'C') & (temp['Delta_c'] < 10) & (temp['Delta_c'] > 0.01))\n",
    "        bl_P = (temp['CallPut'] == 'P') & (temp['Delta_c'] > -10) & (temp['Delta_c'] < -0.01)\n",
    "        temp = temp[bl_C | bl_P]\n",
    "        \n",
    "        temp = merge_syn(temp, securityid)\n",
    "        ############# Calculating result using pre-bubble data ####################\n",
    "        df_MSHE = pd.DataFrame()\n",
    "        exist_coef = os.path.exists(output_path + 'coef/coef_' + str(i) + '.csv')\n",
    "        exist_mshe = os.path.exists(output_path + 'MSHE/MSHE_' + str(i) + '.csv')\n",
    "        \n",
    "        if (exist_coef == True) & (exist_mshe == True):\n",
    "            print('Stock '+ str(i) + ' MSHE result has existed already')        \n",
    "            pass          \n",
    "        else:\n",
    "            df_PNL = pd.DataFrame()\n",
    "            dict_output = {} \n",
    "            \n",
    "            all_date = np.unique(temp.loc[:, 'Date'])\n",
    "            train_date = all_date[0:train_length]\n",
    "            test_date = all_date[train_length:]             \n",
    "            df_test = temp[temp['Date'].isin(test_date)] \n",
    "            if len(test_date) == 0:\n",
    "                print('insufficient observations because length of test date is 0')\n",
    "                continue\n",
    "            if all_date[0] < pd.to_datetime('2020-01-01'):\n",
    "                pre_train_date = [t for t in all_date if t < pd.to_datetime('2020-01-01')] \n",
    "                pre_test_date = [t for t in all_date if (pd.to_datetime('2020-01-01') < t) & (t < test_date[0])] \n",
    "                df_pre_test = temp[temp['Date'].isin(pre_test_date)]       \n",
    "\n",
    "                for s in range(int(np.ceil(len(pre_test_date)/step))):\n",
    "                    if len(pre_test_date) == 0:\n",
    "                        continue                    \n",
    "                    pre_test_date_in = pre_test_date[0:step]\n",
    "                    first_pre_test_date = str(pre_test_date_in[0])[:10]\n",
    "                    df_train_rolling = temp[temp['Date'].isin(pre_train_date)]\n",
    "                    df_test_rolling = temp[temp['Date'].isin(pre_test_date_in)]\n",
    "                    pre_wt_exp = [1.00**i for i in range(len(pre_train_date)-1, -1, -1)]\n",
    "                    df_weight_rolling = pd.DataFrame({'Date':pre_train_date, 'weight':pre_wt_exp})\n",
    "                    \n",
    "                    # print('-------------------------------------------------------------')\n",
    "                    # print('s:', s)\n",
    "                    # print('pre test date:', first_pre_test_date)\n",
    "                    \n",
    "                    df_PNL_one, dict_output_one, dict_plot_one, weights_test = calc_pnl(df_train_rolling, df_test_rolling, df_weight_rolling)\n",
    "                    df_PNL = pd.concat([df_PNL, df_PNL_one])\n",
    "                    dict_output[first_pre_test_date] = dict_output_one\n",
    "\n",
    "                    plot_scatter(dict_plot_one, securityid, date_in = pre_test_date_in, output_path = output_path) \n",
    "                    \n",
    "                    pre_train_date = np.concatenate([pre_train_date, pre_test_date[0:step]])\n",
    "                    pre_test_date = pre_test_date[step:]\n",
    "\n",
    "          \n",
    "            for s in range(int(np.ceil(len(test_date)/step))):\n",
    "                test_date_in = test_date[0:step]\n",
    "                first_test_date = str(test_date_in[0])[:10]\n",
    "                df_train_rolling = temp[temp['Date'].isin(train_date)]\n",
    "                df_test_rolling = temp[temp['Date'].isin(test_date_in)]\n",
    "                df_weight_rolling = pd.DataFrame({'Date':train_date, 'weight':wt_exp})\n",
    "                \n",
    "                # print('test date:', first_test_date)\n",
    "                \n",
    "                df_PNL_one, dict_output_one, dict_plot_one, weights_test = calc_pnl(df_train_rolling, df_test_rolling, df_weight_rolling)\n",
    "                df_PNL = pd.concat([df_PNL, df_PNL_one])\n",
    "                dict_output[first_test_date] = dict_output_one\n",
    "\n",
    "                plot_scatter(dict_plot_one, securityid, date_in = test_date_in, output_path = output_path) \n",
    "                \n",
    "                train_date = np.concatenate([train_date[step:], test_date[0:step]])\n",
    "                test_date = test_date[step:]\n",
    "\n",
    "            if len(df_PNL) == 0:\n",
    "                df_PNL.loc[0, 'CallPut'] = 'C'\n",
    "                df_PNL.loc[0, 'PNL'] = np.nan\n",
    "                df_PNL.loc[1, 'CallPut'] = 'P'\n",
    "                df_PNL.loc[1, 'PNL'] = np.nan \n",
    "            \n",
    "            df_PNL.to_csv(output_path + 'PNL/PNL_' + str(i) + '.csv', index = False)\n",
    "            \n",
    "            df_MSHE.loc[i, \"name\"] = ids[i]\n",
    "            df_MSHE.loc[i, \"bubble_stock\"] = dict_tech_label[i]\n",
    "            df_MSHE.loc[i, \"BS_hedge_C\"] = BS_hedge(df_test, delta_var)[0]\n",
    "            df_MSHE.loc[i, \"BS_hedge_P\"] = BS_hedge(df_test, delta_var)[1]\n",
    "            df_MSHE.loc[i, \"Fixed_hedge_C\"] = Fixed_BS_Hedge(df_test, delta_var)[0]\n",
    "            df_MSHE.loc[i, \"Fixed_hedge_P\"] = Fixed_BS_Hedge(df_test, delta_var)[1]\n",
    "            df_MSHE.loc[i, \"delta_hedge_C\"] = np.mean(df_PNL[df_PNL['CallPut'] == 'C'].loc[:,'PNL']**2)\n",
    "            df_MSHE.loc[i, \"delta_hedge_P\"] = np.mean(df_PNL[df_PNL['CallPut'] == 'P'].loc[:,'PNL']**2)    \n",
    "            df_MSHE.to_csv(output_path + 'MSHE/MSHE_' + str(i) + '.csv', index = False) \n",
    "            df_MSHE_all = pd.concat([df_MSHE_all, df_MSHE])                     \n",
    "\n",
    "            df_coef_ts = pd.DataFrame()\n",
    "            r = 0\n",
    "            for key in dict_output:\n",
    "                df_coef_ts.loc[r, 'Date_str'] = key\n",
    "                df_coef_ts.loc[r, 'Date'] = pd.to_datetime(key)\n",
    "                df_coef_ts.loc[r, 'coef_C'] = dict_output[key]['C']['coef']\n",
    "                df_coef_ts.loc[r, 'std_C'] = dict_output[key]['C']['std']\n",
    "                df_coef_ts.loc[r, 'N_train_C'] = dict_output[key]['C']['N_train']\n",
    "                df_coef_ts.loc[r, 'days_train_C'] = dict_output[key]['C']['days_train']\n",
    "                df_coef_ts.loc[r, 'N_test_C'] = dict_output[key]['C']['N_test']\n",
    "                df_coef_ts.loc[r, 'atm30_vol_test_C'] = dict_output[key]['C']['atm30_vol_test']\n",
    "                df_coef_ts.loc[r, 'atm60_vol_test_C'] = dict_output[key]['C']['atm60_vol_test']\n",
    "                df_coef_ts.loc[r, 'atm90_vol_test_C'] = dict_output[key]['C']['atm90_vol_test']                \n",
    "                df_coef_ts.loc[r, 'atm30_optice_test_C'] = dict_output[key]['C']['atm30_oprice_test']\n",
    "                df_coef_ts.loc[r, 'atm60_optice_test_C'] = dict_output[key]['C']['atm60_oprice_test']   \n",
    "                df_coef_ts.loc[r, 'atm90_optice_test_C'] = dict_output[key]['C']['atm90_oprice_test']                 \n",
    "\n",
    "                df_coef_ts.loc[r, 'coef_P'] = dict_output[key]['P']['coef']\n",
    "                df_coef_ts.loc[r, 'std_P'] = dict_output[key]['P']['std']\n",
    "                df_coef_ts.loc[r, 'N_train_P'] = -dict_output[key]['P']['N_train']\n",
    "                df_coef_ts.loc[r, 'days_train_P'] = dict_output[key]['P']['days_train']\n",
    "                df_coef_ts.loc[r, 'N_test_P'] = dict_output[key]['P']['N_test']   \n",
    "                df_coef_ts.loc[r, 'atm30_vol_test_P'] = dict_output[key]['P']['atm30_vol_test']\n",
    "                df_coef_ts.loc[r, 'atm60_vol_test_P'] = dict_output[key]['P']['atm60_vol_test']\n",
    "                df_coef_ts.loc[r, 'atm90_vol_test_P'] = dict_output[key]['P']['atm90_vol_test']                \n",
    "                df_coef_ts.loc[r, 'atm30_optice_test_P'] = dict_output[key]['P']['atm30_oprice_test']\n",
    "                df_coef_ts.loc[r, 'atm60_optice_test_P'] = dict_output[key]['P']['atm60_oprice_test'] \n",
    "                df_coef_ts.loc[r, 'atm90_optice_test_P'] = dict_output[key]['P']['atm90_oprice_test']                 \n",
    "                r += 1\n",
    "\n",
    "            df_coef_ts = df_coef_ts.merge(temp[['Date', 'S0', 'AdjClosePrice', 'AdjClosePrice2']].drop_duplicates(), how = 'left', on = 'Date')\n",
    "            df_coef_ts.to_csv(output_path + 'coef/coef_' + str(i) + '_ts.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hedging Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MSHE = df_MSHE_all.copy()\n",
    "df_MSHE[\"delta_C_pct_chg_MSHE\"] = df_MSHE[\"delta_hedge_C\"] / df_MSHE[\"BS_hedge_C\"] - 1\n",
    "df_MSHE[\"delta_P_pct_chg_MSHE\"] = df_MSHE[\"delta_hedge_P\"] / df_MSHE[\"BS_hedge_P\"] - 1\n",
    "df_MSHE[\"Fixed_C_pct_chg_MSHE\"] = df_MSHE[\"Fixed_hedge_C\"]/ df_MSHE[\"BS_hedge_C\"] - 1\n",
    "df_MSHE[\"Fixed_P_pct_chg_MSHE\"] = df_MSHE[\"Fixed_hedge_P\"]/ df_MSHE[\"BS_hedge_P\"] - 1\n",
    "df_MSHE = df_MSHE.dropna()\n",
    "df_MSHE.to_csv(output_path + 'MSHE_GME_AMC.csv', index = 'True')\n",
    "    \n",
    "print(\"whole:\")\n",
    "print(\"Number of stocks in total:\", len(df_MSHE))\n",
    "print(\"Number of stocks which has less MSHE with delta hedge (Call)\", len(df_MSHE[df_MSHE['delta_C_pct_chg_MSHE'] < 0.0]))\n",
    "print(\"Number of stocks which has less MSHE with delta hedge (Put)\", len(df_MSHE[df_MSHE['delta_P_pct_chg_MSHE'] < 0.0]))\n",
    "print(\"Number of stocks which has less MSHE with Fixed hedge (Call)\", len(df_MSHE[df_MSHE['Fixed_C_pct_chg_MSHE'] < 0.0]))\n",
    "print(\"Number of stocks which has less MSHE with Fixed hedge (Put)\", len(df_MSHE[df_MSHE['Fixed_P_pct_chg_MSHE'] < 0.0]))\n",
    "df_MSHE.sort_values(by = [\"bubble_stock\", \"delta_C_pct_chg_MSHE\"], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
